# -*- coding: utf-8 -*-
"""bert_baseline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/TurkuNLP/paraphrases_as_question_answering_task/blob/master/Models_and_data/Baselines/bert_baseline.ipynb
"""


import json
import numpy as np
import re
import string
import collections
import sys
import transformers
import torch
import os
import hashlib
import tqdm
import argparse

from metrics import average_f1_score, calculate_exact_match



def embed(data, model, method="AVG", gpu=False):
  with torch.no_grad():
      if gpu:
          input_ids=data["input_ids"].cuda()
          token_type_ids=data["token_type_ids"].cuda()
          attention_mask=data["attention_mask"].cuda()
          spec_mask=data["special_tokens_mask"].cuda()
      else:
          input_ids=data["input_ids"]
          token_type_ids=data["token_type_ids"]
          attention_mask=data["attention_mask"]
          spec_mask=data["special_tokens_mask"]

      emb = model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)
      if method == "POOLER":
          return emb.pooler_output
      elif method == "CLS":
          last_hidden=emb.last_hidden_state
          cls = last_hidden[:,0,:]
          return cls
      elif method == "AVG":
          last_hidden=emb.last_hidden_state
          attention_mask=attention_mask*(spec_mask*-1+1)
          attention_mask_sum=torch.sum(attention_mask,dim=-1) 
          last_hidden_masked=last_hidden.mul(attention_mask.unsqueeze(-1))
          last_hidden_masked_sum=torch.sum(last_hidden_masked,dim=1)
          last_hidden_mean=torch.div(last_hidden_masked_sum,attention_mask_sum.unsqueeze(-1))
          return last_hidden_mean
      else:
          print("Unknown method", method)
          sys.exit()

def predict(questions, contexts, args):

    # init model
    bert_tokenizer=transformers.BertTokenizer.from_pretrained("TurkuNLP/bert-base-finnish-cased-v1")
    model=transformers.BertModel.from_pretrained("TurkuNLP/bert-base-finnish-cased-v1")
    if args.gpu:
        model=model.cuda() 
    model=model.eval() 
    cosine = torch.nn.CosineSimilarity(dim=1, eps=1e-6)
    
    preds={}
    doc_cache = {}


    for key in tqdm.tqdm(questions):
    
        if args.max_examples != 0 and len(preds) > args.max_examples:
            break

        docs = contexts[key]
        q = questions[key]
  
        # EMBED QUESTION
        q_tokenized = bert_tokenizer(q, padding=True, truncation=True, return_tensors="pt", return_special_tokens_mask=True)
        q_emb = embed(q_tokenized, model, method=args.pooling_method, gpu=args.gpu)

        # EMBED CONTEXT
        # same context can repeat many times, cache the embeddings
        c_hash = hashlib.sha256(" ".join(docs).encode('utf-8')).hexdigest()
        if c_hash in doc_cache:
            context_emb = doc_cache[c_hash]
        else:
            d_tokenized = bert_tokenizer(docs, padding=True, truncation=True, return_tensors="pt", return_special_tokens_mask=True)
            context_emb = embed(d_tokenized, model, method=args.pooling_method, gpu=args.gpu)
            doc_cache[c_hash] = context_emb


        # CALCULATE SIMILARITY
        if args.gpu:
            q_emb = q_emb.cpu()
            context_emb = context_emb.cpu()
        similarity = cosine(q_emb, context_emb)
        similarity = similarity.numpy()
        max_index = np.argmax(similarity)
        best_segment = docs[max_index]
        preds[key] = best_segment

    return preds


def main(args):

    # data

    with open(os.path.join(args.eval_data_dir, 'contexts.json'), 'rt', encoding="utf-8") as json_file:
        contexts = json.load(json_file)

    with open(os.path.join(args.eval_data_dir, 'questions.json'), 'rt', encoding="utf-8") as json_file:
        questions = json.load(json_file)

    print("Number of examples:", len(questions))
    assert(len(contexts)==len(questions))
    
    
    # predict
    preds = predict(questions, contexts, args)
    
    # eval
    
    # READ GOLD DATA
    missing = 0 # allow eval on partial predictions for debugging purposes
    with open(os.path.join(args.eval_data_dir, 'answers.json'), 'r') as json_file:
        answers = json.load(json_file)
    d = {}
    for key in answers:
        if key not in preds:
            missing += 1
            continue
        d[key] = answers[key]
    answers = d
    if missing > 0:
        print("Warning!! Missing predictions:", missing)
    
    f1 = average_f1_score(answers, preds)
    print("F-score:", f1)

    em = calculate_exact_match(answers, preds)
    print("Exact match:", em)

if __name__=="__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--eval_data_dir', type=str, required=True)
    parser.add_argument('--pooling-method', default="AVG", help=" Options: AVG, POOLER, or CLS.", type=str)
    parser.add_argument('--max-examples', default=0, type=int, help="Number of examples to run, use 0 for all.")
    parser.add_argument('--gpu', default=False, action="store_true", help="Use gpu, default False.")
    
    args = parser.parse_args()
    
    main(args)

