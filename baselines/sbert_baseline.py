# -*- coding: utf-8 -*-
"""bert_baseline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/TurkuNLP/paraphrases_as_question_answering_task/blob/master/Models_and_data/Baselines/bert_baseline.ipynb
"""


import json
import numpy as np
import sys
import transformers
import torch
import os
import hashlib
import tqdm
import argparse
from sentence_transformers import SentenceTransformer

from metrics import average_f1_score, calculate_exact_match


def predict(questions, contexts, args):

    # init model
    model = SentenceTransformer(args.sbert)

    cosine = torch.nn.CosineSimilarity(dim=1, eps=1e-6)
    
    preds={}
    doc_cache = {}

    for key in tqdm.tqdm(questions):
    
        if args.max_examples != 0 and len(preds) > args.max_examples:
            break

        docs = contexts[key]
        q = questions[key]
  
        # EMBED QUESTION
        q_emb = model.encode(q)

        # EMBED CONTEXT
        # same context can repeat many times, cache the embeddings
        c_hash = hashlib.sha256(" ".join(docs).encode('utf-8')).hexdigest()
        if c_hash in doc_cache:
            context_emb = doc_cache[c_hash]
        else:
            context_emb = model.encode(docs)
            doc_cache[c_hash] = context_emb


        # CALCULATE SIMILARITY
        similarity = cosine(torch.tensor(q_emb), torch.tensor(context_emb))
        similarity = similarity.numpy()
        max_index = np.argmax(similarity)
        best_segment = docs[max_index]
        preds[key] = best_segment

    return preds


def main(args):

    # data

    with open(os.path.join(args.eval_data_dir, 'contexts.json'), 'rt', encoding="utf-8") as json_file:
        contexts = json.load(json_file)

    with open(os.path.join(args.eval_data_dir, 'questions.json'), 'rt', encoding="utf-8") as json_file:
        questions = json.load(json_file)

    print("Number of examples:", len(questions))
    assert(len(contexts)==len(questions))
    
    
    # predict
    preds = predict(questions, contexts, args)
    
    # eval
    
    # READ GOLD DATA
    missing = 0 # allow eval on partial predictions for debugging purposes
    with open(os.path.join(args.eval_data_dir, 'answers.json'), 'r') as json_file:
        answers = json.load(json_file)
    d = {}
    for key in answers:
        if key not in preds:
            missing += 1
            continue
        d[key] = answers[key]
    answers = d
    if missing > 0:
        print("Warning!! Missing predictions:", missing)
    
    f1 = average_f1_score(answers, preds)
    print("F-score:", f1)

    em = calculate_exact_match(answers, preds)
    print("Exact match:", em)

if __name__=="__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--eval_data_dir', type=str, required=True)
    parser.add_argument('--max-examples', default=0, type=int, help="Number of examples to run, use 0 for all.")
    parser.add_argument('--sbert', default="TurkuNLP/sbert-cased-finnish-paraphrase", help="Name of the SBERT model")
    
    args = parser.parse_args()
    
    main(args)

